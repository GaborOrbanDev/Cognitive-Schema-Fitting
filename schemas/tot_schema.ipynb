{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from operator import add\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.runnables import RunnableSerializable, RunnableLambda, RunnableParallel, RunnablePassthrough, RunnablePick\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "import openai\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    \"\"\"`Task` object repesents a general but specific subproblem, decomoposed from the orginal `input_problem` of the user\"\"\"\n",
    "    description: str = Field(..., description=\"Description with essential details of the task\")\n",
    "    \n",
    "\n",
    "class SolvedTask(Task):\n",
    "    \"\"\"`SolvedTask` object represents a task that has been solved by the system\"\"\"\n",
    "    solution: str = Field(..., description=\"The solution of the task\")\n",
    "\n",
    "\n",
    "class SchemaAgentInput(BaseModel):\n",
    "    input_problem: str = Field(..., description=\"The original problem statement given by the user\")\n",
    "    task: Task = Field(..., description=\"The current task to be solved\")\n",
    "    task_history: list[SolvedTask] = Field([], description=\"List of tasks that have been solved so far\")\n",
    "\n",
    "\n",
    "class ThoughtProcess(SchemaAgentInput):\n",
    "    \"\"\"A `ThoughtProcess` object represents the thought process of the system\"\"\"\n",
    "\n",
    "    steps: list[str] = []\n",
    "    thought: Thought | None = None\n",
    "\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    \"\"\"A `Thought` object represents a distinct thought within the cognition of the system\"\"\"\n",
    "\n",
    "    thought: AIMessage\n",
    "    evaluation: float | None = Field(\n",
    "        default=None,\n",
    "        description=\"The evaluation of the thought. It can be a number between 0 and 1.0 being 0 the worst and 1.0 the best.\"\n",
    "    )\n",
    "    context: list[AnyMessage] = []\n",
    "    children: list[Thought] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTAgent:\n",
    "    def __init__(self) -> None:\n",
    "        with open(\"../prompts/tot_prompts.yaml\") as f:\n",
    "            self.prompts = yaml.safe_load(f)\n",
    "\n",
    "        self.decomp_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "        # high temperature for more creative responses, low top_p for more likely responses\n",
    "        # source: https://medium.com/@1511425435311/understanding-openais-temperature-and-top-p-parameters-in-language-models-d2066504684f\n",
    "        self.cognition_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.9, top_p=0.5)\n",
    "        self.evaluation_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    def schema_setup(self, state: SchemaAgentInput) -> ThoughtProcess:\n",
    "        class Steps(BaseModel):\n",
    "            analysis: str = Field(..., description=\"Analysis of the task\")\n",
    "            steps: list[str]\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "                    [(\"system\", self.prompts[\"system_prompt\"]), \n",
    "                    (\"user\", self.prompts[\"task_decomposition_prompt\"])]\n",
    "                )\n",
    "        llm = self.decomp_llm.with_structured_output(Steps)\n",
    "        chain = prompt | llm | RunnableLambda(lambda x: getattr(x, \"steps\"))\n",
    "\n",
    "        steps: list[str] = chain.invoke({\n",
    "            \"input_problem\": state.input_problem,\n",
    "            \"task_history\": state.task_history,\n",
    "            \"task\": state.task\n",
    "        })\n",
    "\n",
    "        return {\"steps\": steps}\n",
    "    \n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    def cognition(self, state: ThoughtProcess) -> ThoughtProcess:\n",
    "        cognition_chain = self._create_thought_generation_chain()\n",
    "        evaluation_chain = self._create_evaluation_chain()\n",
    "        \n",
    "        tree = Thought(\n",
    "            thought=AIMessage(\"<SEED>\")\n",
    "        )\n",
    "\n",
    "        def cognition_walk(parent: Thought, level: int):\n",
    "            child1: Thought = cognition_chain.invoke({\n",
    "                \"task\": state.task,\n",
    "                \"step\": state.steps[level],\n",
    "                \"context\": parent.context\n",
    "            })\n",
    "            child2: Thought = cognition_chain.invoke({\n",
    "                \"task\": state.task,\n",
    "                \"step\": state.steps[level],\n",
    "                \"context\": parent.context\n",
    "            })\n",
    "            return evaluation_chain.invoke({\n",
    "                \"task\": state.task,\n",
    "                \"step\": state.steps,\n",
    "                \"thoughts\": [child1, child2]\n",
    "            })\n",
    "        \n",
    "        \n",
    "        return {\"thoughts\": [*cognition_walk(tree, 0)]}\n",
    "\n",
    "\n",
    "    def _create_thought_generation_chain(self):\n",
    "        chain = (\n",
    "            ChatPromptTemplate.from_messages([\n",
    "                (\"system\", self.prompts[\"thought_generation_system_prompt\"]),\n",
    "                (\"placeholder\", \"{context}\"),\n",
    "                (\"user\", self.prompts[\"thought_generation_prompt\"])\n",
    "            ])\n",
    "            | RunnableParallel(\n",
    "                response=self.cognition_llm,\n",
    "                context=RunnableLambda(lambda p: p.messages[-1:])\n",
    "            )\n",
    "            | RunnableParallel(\n",
    "                response=RunnablePick(keys=\"response\"),\n",
    "                context=lambda x: x[\"context\"] + [x[\"response\"]]\n",
    "            ) \n",
    "            | RunnableLambda(\n",
    "                lambda x: Thought(\n",
    "                    thought=x[\"response\"],\n",
    "                    context=x[\"context\"]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        return chain\n",
    "    \n",
    "    def _create_evaluation_chain(self):\n",
    "        class EvalResults(BaseModel):\n",
    "            evaluation_text: str = Field(..., description=\"Here you can think before answering\")\n",
    "            scores: list[float] = Field(..., description=\"List of scores [0-1], in the order of thoughts within the context\")\n",
    "\n",
    "        chain = (\n",
    "            RunnableLambda(\n",
    "                lambda x: {\n",
    "                    **x,\n",
    "                    \"context\": [t.thought for t in x[\"thoughts\"]]\n",
    "                } \n",
    "            )\n",
    "            | RunnableParallel(\n",
    "                evaluation=(\n",
    "                    ChatPromptTemplate.from_messages(\n",
    "                        [\n",
    "                            (\"user\", self.prompts[\"evaluation_system_prompt\"]),\n",
    "                            (\"placeholder\", \"{context}\"),\n",
    "                            (\"user\", self.prompts[\"evaluation_prompt\"])\n",
    "                        ]\n",
    "                    ) \n",
    "                    | ChatOpenAI(model=\"gpt-4o\", temperature=0.2).with_structured_output(EvalResults)\n",
    "                ),\n",
    "                thoughts=lambda x: x[\"thoughts\"]\n",
    "            )\n",
    "            | RunnableParallel(\n",
    "                evaluation=lambda x: x[\"evaluation\"].scores,\n",
    "                thoughts=lambda x: x[\"thoughts\"]\n",
    "            )\n",
    "            | RunnableLambda(\n",
    "                lambda x: [\n",
    "                    Thought(\n",
    "                        thought=x[\"thoughts\"][i].thought,\n",
    "                        evaluation=score,\n",
    "                        context=x[\"thoughts\"][i].context,\n",
    "                        children=x[\"thoughts\"][i].children\n",
    "                    )\n",
    "                    for i, score in enumerate(x[\"evaluation\"])\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return chain\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../prompts/tot_prompts.yaml\") as f:\n",
    "    PROMPTS = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ToTAgent()\n",
    "\n",
    "workflow = StateGraph(ThoughtProcess, input=SchemaAgentInput)\n",
    "workflow.add_node(\"setup\", agent.schema_setup)\n",
    "workflow.add_node(\"cognition\", agent.cognition)\n",
    "workflow.add_edge(START, \"setup\")\n",
    "workflow.add_edge(\"setup\", \"cognition\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke(SchemaAgentInput(\n",
    "    input_problem=\"is tomato puree and tomato sauce the same thing?\",\n",
    "    task=Task(description=\"Find the difference between tomato puree and tomato sauce\"),\n",
    "    task_history=[]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_problem': 'is tomato puree and tomato sauce the same thing?',\n",
      " 'steps': ['Identify the ingredients and preparation process for tomato puree.',\n",
      "           'Identify the ingredients and preparation process for tomato sauce.',\n",
      "           'Compare the consistency and texture of tomato puree and tomato '\n",
      "           'sauce.',\n",
      "           'Determine the typical culinary uses of tomato puree and tomato '\n",
      "           'sauce.'],\n",
      " 'task': Task(description='Find the difference between tomato puree and tomato sauce'),\n",
      " 'task_history': []}\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
